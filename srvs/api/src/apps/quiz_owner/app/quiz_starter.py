import logging
import numpy as np
import voyageai
from sklearn.feature_extraction.text import CountVectorizer
from hdbscan import HDBSCAN
from umap import UMAP
from bertopic import BERTopic
from bertopic.backend import BaseEmbedder
from bertopic.vectorizers import ClassTfidfTransformer
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance

from src.lib.config import LLMS
from src.lib.settings import settings
from src.apps.material_owner.domain._in import MaterialApp, SearchCmd
from src.apps.user_owner.domain._in import Principal

from ..domain._in import QuizStarter, GenerateCmd
from ..domain.out import QuizIndexer, QuizRepository
from ..domain.models import Quiz
from ..domain.constants import SUMMARY_TOKEN_LIMIT
from ..adapters.out.quiz_preprocesser import QuizPreprocessor

logger = logging.getLogger(__name__)


class VoyageEmbedder(BaseEmbedder):
    """Voyage AI embedder for BERTopic - inherits from BaseEmbedder"""

    def __init__(self, model: str = "voyage-3.5-lite", api_key: str | None = None):
        super().__init__()
        self.client = voyageai.Client(api_key=api_key or settings.voyage_api_key)
        self.model = model

    def embed(self, documents: list[str], verbose: bool = False) -> np.ndarray:
        """Synchronous embed method required by BaseEmbedder"""
        if not documents:
            return np.array([], dtype=np.float32)

        batch_size = 128
        all_embeddings = []

        for i in range(0, len(documents), batch_size):
            batch = documents[i : i + batch_size]
            result = self.client.embed(
                batch,
                model=self.model,
                input_type="document",
            )
            all_embeddings.extend(result.embeddings)

        embeddings = np.array(all_embeddings, dtype=np.float32)
        return embeddings


class QuizStarterImpl(QuizStarter):
    def __init__(
        self,
        quiz_repository: QuizRepository,
        material_app: MaterialApp,
        quiz_indexer: QuizIndexer,
        quiz_preprocessor: QuizPreprocessor,
    ):
        self._material_app = material_app
        self._quiz_indexer = quiz_indexer
        self._quiz_repository = quiz_repository
        self._quiz_preprocessor = quiz_preprocessor

    async def start(self, cmd: GenerateCmd) -> None:
        quiz = await self._quiz_repository.get(cmd.quiz_id)
        quiz.to_preparing()
        await self._quiz_repository.update(quiz)

        logger.info(f"Building table of contents for quiz {quiz.id}")
        await self._build_table_of_contents(quiz, cmd.user)

        topics_vectors = None
        if quiz.query and len(quiz.materials) > 0:
            logger.info(f"Preprocessing query for quiz {quiz.id}")
            topics_vectors = await self._preprocess_query(quiz)

        logger.info(f"Building cluster vectors for quiz {quiz.id}")

        if topics_vectors:
            logger.info(f"Using {len(topics_vectors)} topic vectors from preprocessor")
            quiz.set_cluster_vectors(topics_vectors)
        elif len(quiz.materials) > 0:
            await self._build_cluster_vectors(quiz, cmd.user)
        else:
            quiz.set_cluster_vectors([])

        if quiz.avoid_repeat:
            logger.info(f"Building quiz summary for quiz {quiz.id}")
            await self._build_quiz_summary(quiz, cmd.user)

            logger.info(f"Searching for similar quizes for quiz {quiz.id}")
            similar_quizes = await self._quiz_indexer.search(
                user_id=quiz.author_id,
                query=self._build_query(quiz),
                limit=10,
                ratio=0.5,
                threshold=0.0,
            )

            logger.info(
                f"Found {len(similar_quizes)} similar quizes for quiz {quiz.id}"
            )

            quiz.merge_similar_quizes(similar_quizes)

        quiz.to_creating()
        quiz.increment_generation()
        await self._quiz_repository.update(quiz)

    async def _build_cluster_vectors(self, quiz: Quiz, user: Principal) -> None:
        """
        Build cluster vectors using BERTopic algorithm with pre-computed embeddings:
        1. Get embeddings from chunks (already computed)
        2. Use BERTopic with UMAP + HDBSCAN for clustering
        3. Extract topic representations with c-TF-IDF
        """
        chunks = await self._material_app.search(
            SearchCmd(
                user=user,
                material_ids=[m.id for m in quiz.materials],
                all_chunks=True,
            )
        )

        # Filter chunks with vectors and content
        chunks_with_vectors = [
            c
            for c in chunks
            if c.vector is not None and c.content and c.content.strip()
        ]
        logger.info(
            f"Found {len(chunks_with_vectors)} chunks with vectors and content for quiz {quiz.id}"
        )

        if not chunks_with_vectors:
            logger.warning(
                f"No valid chunks found for quiz {quiz.id}, setting empty cluster vectors"
            )
            quiz.set_cluster_vectors([])
            return

        # Extract embeddings and documents
        embeddings = np.array([c.vector for c in chunks_with_vectors], dtype=np.float32)
        documents = [c.content.strip() for c in chunks_with_vectors]

        n_samples = len(documents)
        n_features = embeddings.shape[1]

        logger.info(
            f"Starting BERTopic for quiz {quiz.id}: {n_samples} documents, {n_features} embedding dimensions"
        )

        # If not enough chunks, use all available vectors
        if n_samples < quiz.length:
            logger.warning(
                f"Not enough chunks for quiz {quiz.id}: found {n_samples} chunks, "
                f"but quiz length is {quiz.length}. Using all available vectors."
            )
            quiz.set_cluster_vectors(embeddings.tolist())
            return

        # Configure BERTopic components
        # Step 1: UMAP for dimensionality reduction
        umap_components = min(5, n_samples - 1)
        n_neighbors = min(15, n_samples - 1)
        logger.info(
            f"Configuring UMAP: {n_features} â†’ {umap_components} dimensions (n_neighbors={n_neighbors})"
        )

        umap_model = UMAP(
            n_neighbors=n_neighbors,
            n_components=umap_components,
            min_dist=0.0,
            metric="cosine",
            random_state=505,
        )

        # Step 2: HDBSCAN for clustering
        min_cluster_size = max(2, min(15, n_samples // 10))
        logger.info(f"Configuring HDBSCAN: min_cluster_size={min_cluster_size}")

        hdbscan_model = HDBSCAN(
            min_cluster_size=min_cluster_size,
            metric="euclidean",
            cluster_selection_method="eom",
            prediction_data=True,
        )

        # Step 3: CountVectorizer for tokenization
        vectorizer_model = CountVectorizer(
            stop_words="english",
            min_df=1,
            max_df=0.95,
            ngram_range=(1, 2),  # unigrams and bigrams
        )

        # Step 4: c-TF-IDF for topic representation
        ctfidf_model = ClassTfidfTransformer(bm25_weighting=True)

        # Step 5: Create Voyage AI embedder for representation models
        voyage_embedder = VoyageEmbedder(
            model="voyage-3.5-lite", api_key=settings.voyageai_api_key
        )

        # Step 6: Configure multi-aspect topic representations
        main_representation = KeyBERTInspired(
            top_n_words=10,
            nr_repr_docs=5,
            nr_samples=min(500, n_samples),
            nr_candidate_words=100,
            random_state=42,
        )

        aspect_representation = MaximalMarginalRelevance(diversity=0.3)

        representation_model = None
        if settings.env == "local":
            representation_model = {
                "Main": main_representation,
                "Diversity": aspect_representation,
            }

        logger.info(f"Configured multi-aspect representations for quiz {quiz.id}")
        logger.info(f"  - Main: KeyBERTInspired (top_n_words=10)")
        logger.info(f"  - Diversity: MaximalMarginalRelevance (diversity=0.5)")

        # Initialize BERTopic with pre-computed embeddings
        logger.info(f"Initializing BERTopic for quiz {quiz.id}")
        topic_model = BERTopic(
            embedding_model=voyage_embedder,
            min_topic_size=25,  # Ð¼Ð¾Ð¶Ð½Ð¾ ÐºÐ°Ðº Ñ‚Ð¾ Ð¿Ð¾ ÑƒÐ¼Ð½Ð¾Ð¼Ñƒ ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ, ÐŸÐžÐšÐ Ð§Ð¢Ðž ÐŸÐžÐ§Ð•ÐœÐ£ Ð¢Ðž ÐÐ• Ð ÐÐ‘ÐžÐ¢ÐÐ•Ð¢
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            ctfidf_model=ctfidf_model,
            representation_model=None,
            verbose=True,
            calculate_probabilities=True,
        ).fit(documents, embeddings)

        # Fit BERTopic with pre-computed embeddings
        logger.info(f"Fitting BERTopic on {n_samples} documents for quiz {quiz.id}")
        try:

            topics, probs = topic_model.transform(documents, embeddings)
            # Get topic info
            topic_info = topic_model.get_topic_info()
            n_topics = len(topic_info[topic_info["Topic"] != -1])
            n_outliers = len(topic_info[topic_info["Topic"] == -1])

            logger.info(
                f"BERTopic found {n_topics} topics for quiz {quiz.id} (outliers: {n_outliers})"
            )
            logger.info(f"Topic distribution:\n{topic_info.to_string()}")

            logger.info(
                f"ðŸ”‘ KeyBERT-enhanced topic representations for quiz {quiz.id}:"
            )
            for topic_id in sorted(topic_model.get_topics().keys()):
                if topic_id != -1:
                    topic_words = topic_model.get_topic(topic_id)
                    top_5_words = ", ".join([word for word, _ in topic_words[:5]])

                    topic_count = len([t for t in topics if t == topic_id])

                    logger.info(
                        f"  Topic {topic_id} ({topic_count} docs): {top_5_words}"
                    )

            # Get representative vectors for each topic (centroids of documents in each topic)
            centers = []
            unique_topics = [t for t in np.unique(topics) if t != -1]

            for topic_id in unique_topics:
                topic_mask = topics == topic_id
                topic_embeddings = embeddings[topic_mask]
                # Use mean as representative vector
                center = np.mean(topic_embeddings, axis=0)
                centers.append(center.tolist())
                logger.info(
                    f"Topic {topic_id}: {np.sum(topic_mask)} documents, center computed"
                )

            # If we have fewer topics than desired quiz length, add outlier vectors
            # if len(centers) < quiz.length:
            #     logger.info(
            #         f"Only {len(centers)} topics found, but quiz length is {quiz.length}. "
            #         f"Adding outlier vectors."
            #     )
            #     outlier_mask = topics == -1
            #     if np.any(outlier_mask):
            #         outlier_embeddings = embeddings[outlier_mask]
            #         additional_needed = quiz.length - len(centers)
            #         centers.extend(outlier_embeddings[:additional_needed].tolist())
            #         logger.info(
            #             f"Added {min(additional_needed, len(outlier_embeddings))} outlier vectors"
            #         )

            # If still need more, sample from existing vectors
            # if len(centers) < quiz.length:
            #     additional_needed = quiz.length - len(centers)
            #     remaining_vectors = [
            #         v.tolist() for v in embeddings if v.tolist() not in centers
            #     ]
            #     centers.extend(remaining_vectors[:additional_needed])
            #     logger.info(
            #         f"Added {min(additional_needed, len(remaining_vectors))} additional vectors"
            #     )

            # Limit to quiz length
            logger.info(
                f"Final cluster vectors: {len(centers)} vectors for quiz {quiz.id}"
            )

            quiz.set_cluster_vectors(centers)

        except Exception as e:
            logger.error(
                f"Error running BERTopic for quiz {quiz.id}: {str(e)}", exc_info=True
            )
            # Fallback: use first N embeddings
            logger.warning(f"Falling back to using first {quiz.length} embeddings")
            n_vectors = min(quiz.length, len(embeddings))
            quiz.set_cluster_vectors(embeddings[:n_vectors].tolist())

    async def _build_quiz_summary(self, quiz: Quiz, user: Principal) -> None:
        chunks = await self._material_app.search(
            SearchCmd(
                user=user,
                material_ids=[m.id for m in quiz.materials],
                limit_tokens=SUMMARY_TOKEN_LIMIT,
                vectors=quiz.cluster_vectors,
            )
        )

        summary = "\n<CHUNK>\n".join([c.content for c in chunks])
        quiz.set_summary(summary)

        # ÐžÑ‚Ð¼ÐµÑ‡Ð°ÐµÐ¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ñ‡Ð°Ð½ÐºÐ¸
        # chunk_ids = [c.id for c in chunks]
        # await self._material_app.mark_chunks_as_used(chunk_ids)
        # logger.info(f"Marked {len(chunk_ids)} chunks as used for quiz {quiz.id}")

    def _build_query(self, quiz: Quiz):
        return f"""
Quiz title: {quiz.title}
Query: {quiz.query}
Summary: {quiz.summary}    
"""

    async def _build_table_of_contents(self, quiz: Quiz, user: Principal) -> None:
        """
        Build table of contents from materials marked as books.
        Stores TOC in quiz.table_of_contents as {material_id: toc}
        """
        if not quiz.materials:
            logger.info(f"No materials for quiz {quiz.id}, skipping TOC build")
            return

        book_materials = [m for m in quiz.materials if m.is_book]
        if not book_materials:
            logger.info(f"No book materials for quiz {quiz.id}, skipping TOC build")
            return

        logger.info(
            f"Building TOC for quiz {quiz.id} from {len(book_materials)} book materials"
        )

        table_of_contents = {}
        for material_ref in book_materials:
            try:
                material = await self._material_app.get_material(material_ref.id)
                if material and hasattr(material, "contents") and material.contents:
                    table_of_contents[material_ref.id] = material.contents
                    logger.info(
                        f"Added TOC for material {material_ref.id} ({material.title})"
                    )
                else:
                    logger.warning(
                        f"Material {material_ref.id} has no table_of_contents"
                    )
            except Exception as e:
                logger.error(
                    f"Failed to get TOC for material {material_ref.id}: {e}",
                    exc_info=True,
                )

        if table_of_contents:
            quiz.set_table_of_contents(table_of_contents)
            logger.info(
                f"Set TOC for quiz {quiz.id} with {len(table_of_contents)} materials"
            )
        else:
            logger.info(f"No TOC found for quiz {quiz.id}")

    async def _preprocess_query(self, quiz: Quiz) -> list[list[float]] | None:
        """
        Preprocess and enhance the user query using QuizPreprocessor.
        Updates quiz.gen_config with enhanced instructions and topics.
        Returns topic vectors if topics were generated, None otherwise.
        """
        if not quiz.query:
            logger.info(f"No query for quiz {quiz.id}, skipping preprocessing")
            return None

        try:
            cache_key = f"quiz_preprocess_{quiz.id}"
            enhanced_instructions, topics = await self._quiz_preprocessor.enhance_query(
                quiz=quiz,
                cache_key=cache_key,
            )

            if enhanced_instructions:
                quiz.gen_config.additional_instructions.append(enhanced_instructions)
                logger.info(
                    f"Added enhanced instructions to quiz {quiz.id}: {enhanced_instructions}"
                )

            if topics and len(topics) > 0:
                quiz.gen_config.more_on_topic.extend(topics)
                logger.info(
                    f"Added {len(topics)} topics to quiz {quiz.id}: {topics}"
                )

                logger.info(f"Embedding {len(topics)} topics for quiz {quiz.id}")
                topic_vectors = await self._embed_topics(topics)
                logger.info(f"Generated {len(topic_vectors)} topic vectors")
                return topic_vectors

            logger.info(f"Query preprocessing completed for quiz {quiz.id}")
            return None

        except Exception as e:
            logger.error(
                f"Failed to preprocess query for quiz {quiz.id}: {e}",
                exc_info=True,
            )
            logger.warning(f"Continuing without query preprocessing for quiz {quiz.id}")
            return None

    async def _embed_topics(self, topics: list[str]) -> list[list[float]]:
        """
        Embed topics using Voyage AI embedder.
        Returns list of embedding vectors.
        """
        try:
            client = voyageai.Client(api_key=settings.voyageai_api_key)
            result = client.embed(
                topics,
                model="voyage-3.5-lite",
                input_type="document",
            )
            embeddings = [emb for emb in result.embeddings]
            logger.info(f"Successfully embedded {len(embeddings)} topics")
            return embeddings
        except Exception as e:
            logger.error(f"Failed to embed topics: {e}", exc_info=True)
            raise e

