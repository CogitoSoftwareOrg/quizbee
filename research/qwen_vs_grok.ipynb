{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f19a23",
   "metadata": {},
   "source": [
    "### Qwen vs Grok\n",
    "Here we are comparing accuracy and speed of Qwen (together.ai) vs GRok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec133b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, Union\n",
    "import httpx\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../envs/.env', override=True)\n",
    "\n",
    "# Verify keys are loaded\n",
    "TOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "GROK_API_KEY = os.environ.get(\"GROK_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestDeps:\n",
    "    \"\"\"Dependencies for the test agent\"\"\"\n",
    "    query: str\n",
    "\n",
    "\n",
    "class QuizItemAnswer(BaseModel):\n",
    "    answer: str = Field(description=\"The answer text\")\n",
    "    explanation: str = Field(description=\"The explanation text\")\n",
    "    correct: bool = Field(description=\"Whether the answer is correct\")\n",
    "\n",
    "\n",
    "class QuizItemTest(BaseModel):\n",
    "    question: str = Field(description=\"The quiz question text\")\n",
    "    answers: list[QuizItemAnswer] = Field(\n",
    "        description=\"4 answers with only one correct\",\n",
    "        min_length=4,\n",
    "        max_length=4\n",
    "    )\n",
    "\n",
    "\n",
    "class TestOutput(BaseModel):\n",
    "    mode: Literal[\"quiz\"] = \"quiz\"\n",
    "    quiz_items: list[QuizItemTest] = Field(\n",
    "        description=\"Array of quiz items\",\n",
    "        min_length=5,\n",
    "        max_length=5\n",
    "    )\n",
    "\n",
    "\n",
    "async def test_llm_performance(\n",
    "    model: Union[str, OpenAIChatModel],\n",
    "    query: str,\n",
    "    num_runs: int = 3\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Test the performance of an LLM model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model identifier (e.g., \"gpt-4o-mini\", \"qwen-2.5-72b-instruct\") \n",
    "               or an OpenAIChatModel instance for custom configuration\n",
    "        query: The query/prompt to test with\n",
    "        num_runs: Number of times to run the test for averaging\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with performance metrics\n",
    "    \"\"\"\n",
    "    agent = Agent(\n",
    "        model=model,\n",
    "        output_type=TestOutput,\n",
    "        deps_type=TestDeps,\n",
    "    )\n",
    "    \n",
    "    # Get model name for display\n",
    "    model_name = model if isinstance(model, str) else f\"{model.model_name}\"\n",
    "    \n",
    "    timings = []\n",
    "    results = []\n",
    "    errors = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        print(f\"Run {i+1}/{num_runs} for {model_name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = await agent.run(\n",
    "                f\"Generate 5 quiz questions about: {query}\",\n",
    "                deps=TestDeps(query=query)\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            timings.append(elapsed)\n",
    "            # Use .output instead of .data for newer pydantic-ai versions\n",
    "            output = result.output if hasattr(result, 'output') else result.data\n",
    "            results.append(output)\n",
    "            print(f\"  ✓ Completed in {elapsed:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - start_time\n",
    "            errors.append(str(e))\n",
    "            print(f\"  ✗ Failed after {elapsed:.2f}s: {e}\")\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"query\": query,\n",
    "        \"num_runs\": num_runs,\n",
    "        \"successful_runs\": len(results),\n",
    "        \"failed_runs\": len(errors),\n",
    "        \"timings\": timings,\n",
    "        \"avg_time\": sum(timings) / len(timings) if timings else None,\n",
    "        \"min_time\": min(timings) if timings else None,\n",
    "        \"max_time\": max(timings) if timings else None,\n",
    "        \"results\": results,\n",
    "        \"errors\": errors\n",
    "    }\n",
    "\n",
    "\n",
    "async def compare_models(\n",
    "    models: list[Union[str, OpenAIChatModel]],\n",
    "    query: str,\n",
    "    num_runs: int = 3\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compare multiple models on the same query.\n",
    "    \n",
    "    Args:\n",
    "        models: List of model identifiers or OpenAIChatModel instances to compare\n",
    "        query: The query/prompt to test with\n",
    "        num_runs: Number of times to run each model\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Comparing {len(models)} models on query: '{query}'\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for model in models:\n",
    "        model_name = model if isinstance(model, str) else f\"{model.model_name}\"\n",
    "        print(f\"\\nTesting {model_name}...\")\n",
    "        result = await test_llm_performance(model, query, num_runs)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for r in results:\n",
    "        print(f\"\\n{r['model']}:\")\n",
    "        print(f\"  Average time: {r['avg_time']:.2f}s\" if r['avg_time'] else \"  Failed\")\n",
    "        print(f\"  Min time: {r['min_time']:.2f}s\" if r['min_time'] else \"\")\n",
    "        print(f\"  Max time: {r['max_time']:.2f}s\" if r['max_time'] else \"\")\n",
    "        print(f\"  Success rate: {r['successful_runs']}/{r['num_runs']}\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"models\": results,\n",
    "        \"fastest\": min(results, key=lambda x: x['avg_time'] or float('inf'))['model'] if any(r['avg_time'] for r in results) else None\n",
    "    }\n",
    "\n",
    "\n",
    "# Example invocation cell below (keeps the key private; agent implementations will use system env vars):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5f6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create custom model instances for Together.ai and Grok\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "from pydantic_ai.providers.grok import GrokProvider\n",
    "\n",
    "# Qwen model via Together.ai\n",
    "qwen_together = OpenAIChatModel(\n",
    "    \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\",\n",
    "    provider=OpenAIProvider(\n",
    "        base_url=\"https://api.together.xyz/v1\",\n",
    "        api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97570f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Comparing 2 models on query: 'Generate me 1 quiz questions about Python programming basics. '\n",
      "============================================================\n",
      "\n",
      "\n",
      "Testing Qwen/Qwen3-235B-A22B-Instruct-2507-tput...\n",
      "Run 1/2 for Qwen/Qwen3-235B-A22B-Instruct-2507-tput...\n",
      "  ✓ Completed in 33.00s\n",
      "Run 2/2 for Qwen/Qwen3-235B-A22B-Instruct-2507-tput...\n",
      "  ✓ Completed in 29.22s\n",
      "\n",
      "Testing grok:grok-4-fast-non-reasoning...\n",
      "Run 1/2 for grok:grok-4-fast-non-reasoning...\n",
      "  ✓ Completed in 3.82s\n",
      "Run 2/2 for grok:grok-4-fast-non-reasoning...\n",
      "  ✓ Completed in 4.09s\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "Qwen/Qwen3-235B-A22B-Instruct-2507-tput:\n",
      "  Average time: 31.11s\n",
      "  Min time: 29.22s\n",
      "  Max time: 33.00s\n",
      "  Success rate: 2/2\n",
      "\n",
      "grok:grok-4-fast-non-reasoning:\n",
      "  Average time: 3.95s\n",
      "  Min time: 3.82s\n",
      "  Max time: 4.09s\n",
      "  Success rate: 2/2\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Mix of string models and custom OpenAIModel instances\n",
    "results = await compare_models(\n",
    "    models=[\n",
    "       \n",
    "\n",
    "        qwen_together,          # Custom OpenAIModel instance\n",
    "         \"grok:grok-4-fast-non-reasoning\",  # String model identifier\n",
    "    ],\n",
    "    query=\"Generate me 1 quiz questions about Python programming basics. \",\n",
    "    num_runs=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quizbee-workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
